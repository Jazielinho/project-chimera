# Project Chimera üêâ: Construyendo un mini-CLIP en P√∫blico

Este repositorio documenta el sprint de un mes (agosto 2025) para construir un modelo de Visi√≥n-Lenguaje (mini-CLIP) desde un "cero did√°ctico" sobre hardware de consumidor (NVIDIA GTX 1080). El objetivo es doble: explorar las tripas de la IA multimodal y hacerlo siguiendo principios de ingenier√≠a de software robustos y reproducibles.

**Este proyecto es un experimento en `Building in Public`. Todo el progreso, los errores y los aprendizajes ser√°n documentados.**

---

## Principios Operativos (No Negociables)

1.  **SLOs son Ley:** Los objetivos de rendimiento y tiempo se respetan. Si no se cumplen, se activan planes de contingencia documentados.
2.  **Reproducibilidad Total:** Cada resultado debe ser reproducible. Se versiona el c√≥digo, la configuraci√≥n, los datos y el entorno de hardware/software.
3.  **Evaluaci√≥n Honesta:** Las m√©tricas se reportan con rigor estad√≠stico (`media ¬± std` en 3 `seeds`) para evitar celebrar picos de suerte.
4.  **Documentar es Parte del Trabajo:** `KNOWN_ISSUES.md` y `CHANGELOG.md` se actualizan a diario.

---

## üéØ SLOs (Service Level Objectives) y Criterios de √âxito

Estos son los objetivos m√≠nimos para que el proyecto se considere viable.

| M√©trica | Objetivo | C√≥mo se Mide |
|---|---|---|
| **Dataloader Throughput** | ‚â• 300 img/s con GPU util ‚â• 85% | Promedio de 60-120s con `nvidia-smi dmon -s pucm`. Se acepta <300 img/s si `util` se mantiene >85%. |
| **Prueba de Vida (Sanity)** | `loss[0] - loss[200] ‚â• 0.15` en `‚â§ 25 min` | `wall-clock` time para el script `train_sanity.py`. |
| **Tiempo por Epoch (Real)** | `‚â§ 2‚Äì3 h` (para 10k pares) | `wall-clock` time del script de entrenamiento. |
| **VRAM Pico** | `‚â§ 7.5 GB` | `torch.cuda.max_memory_allocated()` logueado en MLflow. |
| **Viabilidad del Modelo (GO/NO-GO #2)** | `recall@10 ‚â• 0.25` y `recall@1 ‚â• 0.05` | Media sobre 3 seeds en el split de validaci√≥n oficial de Flickr8k. |

---

## üõ†Ô∏è Tech Stack y Datos

* **Frameworks:** PyTorch, Transformers, timm
* **Herramientas:** Conda, Conda-Lock, MLflow, Ruff, Black, Pytest
* **CI/CD:** GitHub Actions
* **Datos:** Flickr8k (splits oficiales)
* **Licencia del C√≥digo:** Apache 2.0
* **Licencia de Datos:** Se respetan las licencias originales de Flickr8k/COCO.

---

## üöÄ C√≥mo Reproducir

1.  Clonar el repositorio: `git clone https://github.com/tu-usuario/project-chimera.git`
2.  Instalar dependencias: `make install`
3.  Ejecutar el pipeline de reproducibilidad completo (subset): `make reproduce_small`

---

## üìä Benchmarks y Costes

*(Esta tabla se rellenar√° a medida que avance el proyecto)*

| Componente | Latencia (ms/batch) | Throughput | VRAM Pico (GB) |
|---|---|---|---|
| Dataloader | - | | - |
| Image Encoder | | | |
| Text Encoder | | | |

| Tarea | Tiempo de Ejecuci√≥n (Wall-Clock) |
|---|---|
| Prueba de Vida | |
| Epoch Completo (10k pares) | |